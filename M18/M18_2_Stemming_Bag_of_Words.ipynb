{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f42f72-3cd0-4469-a697-8516cf03250a",
   "metadata": {},
   "source": [
    "### Codio Activity 18.3: Stemming and Lemmatization\n",
    "\n",
    "In this activity, you will stem and lemmatize a text to normalize a given text.  Here, you will review using the lemmatizer and stemmer on a basic list and then turn to data in a DataFrame, writing a function to apply the lemmatization and stemming operations to a column of text data.  The data is the WhatsApp status dataset from kaggle, and you will focus on the `content` feature.\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9a5f9e-ee4a-45ec-9b7e-c962fc3de1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/kellen/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/kellen/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /Users/kellen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078731b-0542-46b4-9383-1b907ec8b669",
   "metadata": {},
   "source": [
    "#### The Data\n",
    "\n",
    "The text data again comes from [kaggle](https://www.kaggle.com/datasets/sankha1998/emotion?select=Emotion%28sad%29.csv) and is related to classifying WhatsApp status. We load in only the \"angry\" sentiment below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70fe99dc-8050-4247-a5c1-40abe2017d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "angry = pd.read_csv('codio_18_3_solution/data/Emotion(angry).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebe835e-9aff-48cc-967c-2a32922e4f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sometimes I‚Äôm not angry, I‚Äôm hurt and there‚Äôs ...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not available for busy people‚ò∫</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I do not exist to impress the world. I exist t...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Everything is getting expensive except some pe...</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My phone screen is brighter than my future üôÅ</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content sentiment\n",
       "0  Sometimes I‚Äôm not angry, I‚Äôm hurt and there‚Äôs ...     angry\n",
       "1                     Not available for busy people‚ò∫     angry\n",
       "2  I do not exist to impress the world. I exist t...     angry\n",
       "3  Everything is getting expensive except some pe...     angry\n",
       "4       My phone screen is brighter than my future üôÅ     angry"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angry.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc792c0-8237-47c1-84a7-5b5975611840",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "#### Stemming a list of words\n",
    "\n",
    "Use `PorterStemmer` to stem the different variations on the word \"compute\" in the list `C` below.  Assign your results to the list `stemmed_words` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc96880e-bc6d-4832-a009-a2387837d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = ['computer', 'computing', 'computed', 'computes', 'computation', 'compute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd3e32a-95a7-4133-9484-07c6d29ae06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730b594e-7ae7-42fa-99f1-4d0953887948",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = [stemmer.stem(i) for i in C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c65aa9b-f156-469d-a3d2-47aef52be940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['comput', 'comput', 'comput', 'comput', 'comput', 'comput']\n"
     ]
    }
   ],
   "source": [
    "print(type(stemmed_words))\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a2c35-63b6-4b76-bfc5-eee257cac9cd",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "#### Lemmatizing a list of words\n",
    "\n",
    "Use `WordNetLemmatizer` to stem the different variations on the word \"compute\" in the list `C` below.  Assign your results to the list `lemmatized_words` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f648b4-4e3e-4b45-b496-5cb387085562",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59e56c67-e332-447a-a22b-402a60c209a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemma.lemmatize(w) for w in C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32501087-affc-48e4-a7c5-96fff7748e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['computer', 'computing', 'computed', 'computes', 'computation', 'compute']\n"
     ]
    }
   ],
   "source": [
    "print(type(lemmatized_words))\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4bc8d-1c2f-4fcf-98ad-c984ecdec746",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "#### Which performed better\n",
    "\n",
    "Assuming we wanted all the words in `C` to be normalized to the same word, which worked better to this goal -- stemming or lemmatizing.  Assign your response as a string -- `stem` or `lemmatize` -- to `ans3` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2af9f54-dcf4-4f4c-935e-a59a20369fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans3 = 'stem'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2956c03-7d43-4410-a59a-fde71a0e91f8",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "#### A function for stemming\n",
    "\n",
    "Use `PorterStemmer` to complete the function `stemmer` below. This function should take in a string of text and return a string of stemmed text. Note that you will need to tokenize the text before stemming and should return a single string.  \n",
    "\n",
    "Hint: use the `join` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9904107-6441-4380-97e3-9ced91a54b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    '''\n",
    "    This function takes in a string of text and returns\n",
    "    a string of stemmed text.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text: str\n",
    "        string of text to be stemmed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "       string of stemmed words from the text input\n",
    "    '''\n",
    "    return ''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a81a577d-f157-4a36-acb3-d58aa07e2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    lst = word_tokenize(text)\n",
    "    stem = PorterStemmer()\n",
    "    stem_lst = [stem.stem(i) for i in lst]\n",
    "    return ' '.join(stem_lst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62834959-516b-4e66-8206-f7f37a2cf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The computer did not compute the answers correctly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcfc3ff8-daae-46c1-8a75-474cb7e80138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the comput did not comput the answer correctli .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89228c40-4664-4ecc-9fe3-b06210f50d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computer did not compute the answers correctly.\n",
      "the comput did not comput the answer correctli .\n"
     ]
    }
   ],
   "source": [
    "def stemmer(text):\n",
    "    stem = PorterStemmer()\n",
    "    return ' '.join([stem.stem(w) for w in word_tokenize(text)])\n",
    "    \n",
    "text = 'The computer did not compute the answers correctly.'\n",
    "print(text)\n",
    "print(stemmer(text))#should return --> the comput did not comput the answer correctli ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0067b-8f10-4d81-8cf9-61068c6e94ab",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "#### Using the stemmer on a DataFrame\n",
    "\n",
    "Use your function `stemmer` to apply to the `content` feature of the DataFrame `angry`.  Assign the resulting series to `stemmed_content` below.\n",
    "\n",
    "Hint: use the `.apply` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6823cfe4-b751-4fe3-a139-ed6faf29c9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "0    sometim i ‚Äô m not angri , i ‚Äô m hurt and there...\n",
      "1                           not avail for busi people‚ò∫\n",
      "2    i do not exist to impress the world . i exist ...\n",
      "3    everyth is get expens except some peopl , they...\n",
      "4          my phone screen is brighter than my futur üôÅ\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stemmed_content = angry['content'].apply(stemmer)\n",
    "print(type(stemmed_content))\n",
    "print(stemmed_content.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc83e16-5408-47f2-8329-9e628de246f9",
   "metadata": {},
   "source": [
    "### Codio Activity 18.4: Bag of Words: Count Vectorization\n",
    "\n",
    "In this activity you will use the scikit-learn vectorization tool `CountVectorizer` to create a bag of words representation of text in a DataFrame.  You will explore how different parameter settings affect the performance of a `LogisticRegression` estimator on a binary classification problem.\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "082a61ee-e3a5-4271-856b-7fc705f136db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c99f77-d14a-423c-b03a-961c714f5ec8",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Below, the data from kaggle is again loaded.  Now, we join the \"sad\" and \"happy\" sentiments which will form the target of our classification models.  The data is also split and named appropriately below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6262abd4-38fa-474d-8d25-d985721aac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_df = pd.read_csv('codio_18_4_solution/data/Emotion(happy).csv')\n",
    "sad_df = pd.read_csv('codio_18_4_solution/data/Emotion(sad).csv.zip', compression = 'zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d54150f5-f7f2-40ee-bd06-00e0caf7c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([happy_df, sad_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43b63ab8-e8ed-44ba-b653-c214b4f5a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_df.drop('sentiment', axis = 1)\n",
    "y = full_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cb49c7a-2e92-4cb1-86f1-754de3a15a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X['content'], y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c82cf80-fae5-4721-9090-d58afbadf6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1287    ['You Hurt Me But I Still Love You.', 'True Lo...\n",
       "1112    Sorry isn‚Äôt always enough. Sometimes you actua...\n",
       "823     Sometimes two people have to fall apart to rea...\n",
       "651     True love isn‚Äôt love at first sight but love a...\n",
       "1101    i am scared of getting too close to anyone bec...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50d585-e76a-4cf2-ab30-787daa78bf8a",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "#### Using the `CountVectorizer`\n",
    "\n",
    "To create a bag of words representation of your text data, create an instance of the `CountVectorizer` as `cvect` below.  Leave all the default settings, and assign the transformed version of the text to `dtm`.  Note that because the vectorizer will return a `scipy.sparse` array, to view the contents of the resulting document term matrix the `toarray()` function is used together with the `.get_feature_names()` function to retrieve the fitted vocabulary.\n",
    "\n",
    "Hint: Make sure to transform X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94f453f1-358c-4d20-b1cf-14a29c15e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7372d9ca-8cb9-4343-8e5b-3bbe8911c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = cvect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "291d8c6e-2835-4873-ba48-66f23f2f2166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_0</th>\n",
       "      <th>100</th>\n",
       "      <th>123whatsappstatus</th>\n",
       "      <th>204</th>\n",
       "      <th>30</th>\n",
       "      <th>404</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>55</th>\n",
       "      <th>805</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yous</th>\n",
       "      <th>yuh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 1832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0_0  100  123whatsappstatus  204  30  404  44  45  55  805  ...  yes  \\\n",
       "0    0    0                  0    0   0    0   0   0   0    0  ...    2   \n",
       "1    0    0                  0    0   0    0   0   0   0    0  ...    0   \n",
       "2    0    0                  0    0   0    0   0   0   0    0  ...    0   \n",
       "3    0    0                  0    0   0    0   0   0   0    0  ...    0   \n",
       "4    0    0                  0    0   0    0   0   0   0    0  ...    0   \n",
       "\n",
       "   yesterday  yet  you  young  your  yours  yourself  yous  yuh  \n",
       "0          0    1  112      0    13      0         2     0    0  \n",
       "1          0    0    1      0     0      0         0     0    0  \n",
       "2          0    0    0      0     0      0         0     0    0  \n",
       "3          0    0    0      0     0      0         0     0    0  \n",
       "4          0    0    0      0     0      0         0     0    0  \n",
       "\n",
       "[5 rows x 1832 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns = cvect.get_feature_names_out()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227467c-894f-4899-9a91-5d0d5217bb29",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "#### Limiting words with the `CountVectorizer`\n",
    "\n",
    "Now, to remove stopwords from the text before vectorizing create a new instance of the `CountVectorizer` and set `stop_words = 'english'` to remove the english language stop words using the same list as in our earlier assignment.  Fit and transform the training data and transform the test data as `X_train_vect_2` and `X_test_vect_2` below.\n",
    "\n",
    "Hint: Use `fit_transform` for the training data, and `transform` for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a91088c-e81e-42c6-bed4-169b8b0e64b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 41589 stored elements and shape (1007, 1622)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect2 = CountVectorizer(stop_words = 'english')\n",
    "X_train_vect_2 = cvect2.fit_transform(X_train)\n",
    "X_test_vect_2 = cvect2.transform(X_test)\n",
    "\n",
    "X_train_vect_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9b690-0c0e-4491-bf71-9ef7a946efe8",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "#### Limiting words with stopwords and higher counts\n",
    "\n",
    "Now, remove stopwords using `stop_words = 'english'` and limit the features to the top 300 words based on counts using the `max_features` argument.  Fit and transform your data appropriately as `X_train_vect_3` and `X_test_vect_3` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60255e1e-a7a4-4071-9f5d-edbbc9183cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 33225 stored elements and shape (1007, 300)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect3 = CountVectorizer(stop_words = 'english', max_features = 300)\n",
    "X_train_vect_3 = cvect3.fit_transform(X_train)\n",
    "X_test_vect_3 = cvect3.transform(X_test)\n",
    "\n",
    "X_train_vect_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092883ab-1236-4528-b941-54fce94e10c7",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "#### Using the text with `LogisticRegression`\n",
    "\n",
    "Create a `Pipeline` object named `vect_pipe_1` below that has steps named `cvect` and `lgr`, using both a default `CountVectorizer` transformer and `LogisticRegression` estimator. Fit this on the training data and evaluate it on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "342466dc-1588-43a4-ab40-daa5956b8351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvect': CountVectorizer(), 'lgr': LogisticRegression()}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_pipe_1 = Pipeline([('cvect', CountVectorizer()),\n",
    "                       ('lgr', LogisticRegression())])\n",
    "vect_pipe_1.fit(X_train, y_train)\n",
    "test_acc = vect_pipe_1.score(X_test, y_test)\n",
    "\n",
    "vect_pipe_1.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed7f14-ef96-42e7-baa0-be8d07296eb4",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "#### Pipeline and Grid Search\n",
    "\n",
    "Finally, to abstract this work into a single step you can create a `Pipeline` with named steps `cvect` and `lgr` below that vectorize and model the data.  Then, use the parameter grid to perform a grid search for the ideal parameters to represent the text and build a classification model. \n",
    "\n",
    "Hint: Use vect_pipe_1 from problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0da19a0b-fe10-4cf2-a663-5ef41dbb0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'cvect__max_features': [100, 500, 1000, 2000],\n",
    "         'cvect__stop_words': ['english', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3740b6ae-9f1b-444a-a553-ecf70f469a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvect__max_features': 2000, 'cvect__stop_words': None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(vect_pipe_1, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "test_acc = grid.score(X_test, y_test)\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56bfdba-8d6b-419e-afd3-565d3ea7aff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
